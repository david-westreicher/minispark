# ðŸ”¬ Inner workings

This section explains what happens inside **minispark** when you run a query â€” from text to final result.

## 1) Start with some data and a query
### Users
```bash
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚   user_id â”‚ first_name   â”‚ last_name   â”‚   age â”‚ country   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         1 â”‚ Alice        â”‚ Smith       â”‚    25 â”‚ USA       â”‚
â”‚         2 â”‚ Bob          â”‚ Johnson     â”‚    30 â”‚ Canada    â”‚
â”‚         3 â”‚ Charlie      â”‚ Brown       â”‚    22 â”‚ USA       â”‚
â”‚         4 â”‚ David        â”‚ Wilson      â”‚    35 â”‚ UK        â”‚
â”‚         5 â”‚ Eva          â”‚ Davis       â”‚    28 â”‚ Canada    â”‚
â”‚         6 â”‚ Frank        â”‚ Miller      â”‚    40 â”‚ USA       â”‚
â”‚         7 â”‚ Grace        â”‚ Taylor      â”‚    27 â”‚ UK        â”‚
â”‚         8 â”‚ Hank         â”‚ Anderson    â”‚    32 â”‚ USA       â”‚
â”‚         9 â”‚ Ivy          â”‚ Thomas      â”‚    26 â”‚ Canada    â”‚
â”‚        10 â”‚ Jack         â”‚ Jackson     â”‚    24 â”‚ USA       â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```
### Orders
```bash
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚   order_id â”‚   user_id â”‚ product   â”‚   quantity â”‚   price â”‚ order_date          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚          1 â”‚         1 â”‚ Laptop    â”‚          1 â”‚    1200 â”‚ 2025-01-01 00:00:00 â”‚
â”‚          2 â”‚         2 â”‚ Mouse     â”‚          2 â”‚      25 â”‚ 2025-01-05 00:00:00 â”‚
â”‚          3 â”‚         3 â”‚ Keyboard  â”‚          1 â”‚      45 â”‚ 2025-02-10 00:00:00 â”‚
â”‚          4 â”‚         1 â”‚ Monitor   â”‚          2 â”‚     300 â”‚ 2025-03-15 00:00:00 â”‚
â”‚          5 â”‚         4 â”‚ Laptop    â”‚          1 â”‚    1100 â”‚ 2025-03-20 00:00:00 â”‚
â”‚          6 â”‚         5 â”‚ Mouse     â”‚          1 â”‚      30 â”‚ 2025-04-01 00:00:00 â”‚
â”‚          7 â”‚         6 â”‚ Keyboard  â”‚          2 â”‚      50 â”‚ 2025-04-10 00:00:00 â”‚
â”‚          8 â”‚         7 â”‚ Monitor   â”‚          1 â”‚     280 â”‚ 2025-05-05 00:00:00 â”‚
â”‚          9 â”‚         8 â”‚ Laptop    â”‚          1 â”‚    1300 â”‚ 2025-05-10 00:00:00 â”‚
â”‚         10 â”‚         9 â”‚ Mouse     â”‚          3 â”‚      27 â”‚ 2025-06-01 00:00:00 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

### Query:
```sql
SELECT u.country, COUNT() AS orders_count, SUM(o.quantity*o.price) AS total_sales
FROM 'users' AS u
    JOIN 'orders' AS o ON u.user_id=o.user_id
GROUP BY u.country
HAVING SUM(o.quantity*o.price) > 500;
```

## 2) Parsing (PEG â€” Parsimonious)

- The SQL text is passed to a **PEG parser** implemented with [Parsimonious](https://github.com/erikrose/parsimonious).  
- The result of this parsing step is a *DataFrame* object that represents the query in a structured way.

```python
df = (
    DataFrame()
    .table('users').alias('u')
    .join(
        DataFrame().table('orders').alias('o'),
        on=Col('u.user_id') == Col('o.user_id'),
        how="inner"
    )
    .group_by(Col('u.country'))
    .agg(
        F.count().alias('orders_count'),
        F.sum(Col('o.quantity') * Col('o.price')).alias('total_sales')
    )
    .filter(F.col("total_sales") > 500)
    .select(Col("u.country"), Col("orders_count"), Col("total_sales"))
)
```

Notice that the translation from SQL to the *DataFrame* is not straightforward:

- Selections are done in the end
- `HAVING` conditions are done after the aggregation
- `COUNT` and `SUM` appear in the select statement but need to be computed during aggregation

## 3) Logical Plan

This dataframe is now converted into a logical plan

```python
 Project(u.country, orders_count, total_sales):None
  +-  Filter((_having_sum_o.quantity_mul_o.price) > (500)):None
    +-  AggregateTask(group_by: u.country, agg: [
            AggCol(original_col=Lit(value=1), name='orders_count', type='sum'),
            AggCol(original_col=BinaryOperatorColumn(left_side=o.quantity, right_side=o.price, operator=<built-in function mul>, left_type_convert_to=None, right_type_convert_to=None), name='total_sales', type='sum'),
            AggCol(original_col=BinaryOperatorColumn(left_side=o.quantity, right_side=o.price, operator=<built-in function mul>, left_type_convert_to=None, right_type_convert_to=None), name='_having_sum_o.quantity_mul_o.price', type='sum')
        ]):None
      +-  JoinTask((u.user_id) == (o.user_id), "inner"):None
        +-  LoadTableBlockTask(users):None
        +-  LoadTableBlockTask(orders):None
```

This format is similar to what you would find in other query engines like Spark or DuckDB.
You read it from the bottom up, indentations indicate data flow, the `:none` at the end will be explained soon.

- 2 `LoadTableBlock` tasks read the `users` and `orders` tables
- The `JoinTask` combines them into a new intermediate table
- The `AggregateTask` groups by country and computes the aggregations
  - Notice that there are 3 aggregations
  - The last one is an internal one used to compute the `HAVING` condition
  - Each aggregation stores the computation needed to compute it
    - `orders_count` is a `sum` over the literal `1` -> this computes the count
    - `total_sales` is a `sum` over the expression (`BinaryOperatorColumn`) `o.quantity * o.price`
- The `Filter` task applies the `HAVING` condition
- The `Project` task selects the final columns to return


## 4) Physical Plan

The logical plan is converted into a Physical Plan.
A physical plan consists of multiple stages that will be executed one after the other (a barrier, where all workers need to wait until the stage is finished, to continue). Each stage will output one or more files that will be read by the next stage.
Each worker has a dedicated folder to write its (output/shuffle) files to, so that they can safely run in parallel.
This plan now also contains schema information from the source tables and their propagated schemas.

```python
# Stage 0
 WriteToShufflePartitions(u.user_id)
  schema = [u.user_id:INTEGER, u.first_name:STRING, u.last_name:STRING, u.age:INTEGER, u.country:STRING]
  +-  LoadTableBlockTask(users)
       schema = [u.user_id:INTEGER, u.first_name:STRING, u.last_name:STRING, u.age:INTEGER, u.country:STRING]
----------
# Stage 1
 WriteToShufflePartitions(o.user_id)
  schema = [o.order_id:INTEGER, o.user_id:INTEGER, o.product:STRING, o.quantity:INTEGER, o.price:FLOAT, o.order_date:TIMESTAMP]
  +-  LoadTableBlockTask(orders)
       schema = [o.order_id:INTEGER, o.user_id:INTEGER, o.product:STRING, o.quantity:INTEGER, o.price:FLOAT, o.order_date:TIMESTAMP]
----------
# Stage 2
 WriteToShufflePartitions(u.country)
  schema = [u.country:STRING, orders_count:INTEGER, total_sales:FLOAT, _having_sum_o.quantity_mul_o.price:FLOAT]
  +-  AggregateTask(group_by: u.country, agg: [...], before_shuffle:True)
       schema = [u.country:STRING, orders_count:INTEGER, total_sales:FLOAT, _having_sum_o.quantity_mul_o.price:FLOAT]
    +-  Join((u.user_id) == (o.user_id), "inner")
         schema = [u.user_id:INTEGER, u.first_name:STRING, u.last_name:STRING, u.age:INTEGER, u.country:STRING,
                   o.order_id:INTEGER, o.user_id:INTEGER, o.product:STRING, o.quantity:INTEGER, o.price:FLOAT, o.order_date:TIMESTAMP]
----------
# Stage 3
 WriteToLocalFileTask():[country:STRING, orders_count:INTEGER, total_sales:FLOAT]
  +-  Project((u.country) AS country, (orders_count) AS orders_count, (total_sales) AS total_sales)
       schema = [country:STRING, orders_count:INTEGER, total_sales:FLOAT]
    +-  Project(u.country, orders_count, total_sales)
         schema = [u.country:STRING, orders_count:INTEGER, total_sales:FLOAT]
      +-  Filter((_having_sum_o.quantity_mul_o.price) > (500))
           schema = [u.country:STRING, orders_count:INTEGER, total_sales:FLOAT, _having_sum_o.quantity_mul_o.price:FLOAT]
        +-  AggregateTask(group_by: u.country, agg: [...], before_shuffle:False)
             schema = [u.country:STRING, orders_count:INTEGER, total_sales:FLOAT, _having_sum_o.quantity_mul_o.price:FLOAT]
          +-  LoadShuffleFile()
               schema = [u.country:STRING, orders_count:INTEGER, total_sales:FLOAT, _having_sum_o.quantity_mul_o.price:FLOAT]
----------
```


- **Stage 0**: Load the `users` table and distribute rows into shuffle partitions `left.partition_i` based on `user_id` (the join key)
- **Stage 1**: Load the `orders` table and distribute rows into shuffle partitions `right.partition_i` based on `user_id` (the join key)
- **Stage 2**:
    - Choose a partition `i` based on the job (will be explained later)
    - Load the shuffled data from the left side (read full `left.partition_i`)
    - Load the shuffled data from the right side (read block by block from `right.partition_i`)
    - Perform the `JOIN` (using a hash join)
    - Do a local aggregation by `country` and write results to shuffle partitions `worker_j_agg_i` based on `country` (the group by key)
- **Stage 3**:
    - Choose a partition `i` based on the job (will be explained later)
    - Read shuffle files from all workers `worker_x_agg_i`
    - Aggregate the data by `country` (final aggregation)
    - Filter results based on the `HAVING` condition
    - Project the final columns (notice that we remove the table alias names)
    - Write the final results to a local file

Notice that we do the aggregation twice. First we *pre-aggregate* the data so that the shuffle files are smaller. Then we do the final aggregation after the shuffle.

## 5) Job Creation
A job in **minispark** corresponds to running a stage for one part of the input data. Depending on the execution engine, jobs can be run sequentially or in parallel.
In our example the following jobs would be run:

- **Stage 0**: Create a job for each partition of the `users` table, load the data and write to shuffle partitions based on `user_id` (block by block) **`ScanJob(file=..., block_id=...)`**
- **Stage 1**: Create a job for each partition of the `orders` table, load the data and write to shuffle partitions based on `user_id` (block by block) **`ScanJob(file=..., block_id=...)`**
- **Stage 2**: The *driver* collects the locations of shuffle partitions created in stages 0 and 1. Each partition `i` creates a job containing the shuffle files from the left side and the shuffle files from the right side (with the same partition number). **`JoinJob(left_shuffle_files=..., right_shuffle_files=..., parition=i)`**
- **Stage 3**: The *driver* collects the list of shuffle partitions created in stage 2 and bundles them per partition `i` **`LoadShuffleFilesJob(files=..., partition=i)`**.
- **Finally**: The *driver* collects the output files from stage 3 and streams them to the user (`collect` / `show`).

## 6) Execution

The execution engine takes care of running the jobs. Depending on the engine, jobs can be run sequentially (PythonEngine) or in parallel (ThreadPoolEngine).
The *driver* coordinates the job creation (per stage), execution of the stages and collection of results.
Each stage is a *chunk* pipeline, starting with a *Producer*, leading to *Consumers* and ending with a *Writer*.

- Producer: `JoinTask`, `LoadTableBlockTask`, `LoadShuffleFile`
- Consumers: `AggregateTask`, `Filter`, `Project`
- Writer: `WriteToShufflePartitions`, `WriteToLocalFileTask`

We try to chunk the data so that we don't run out of memory. The `AggregateTask` / `JoinTask` however, need to keep an in-memory hash map of the full partition it processes (left-partition for the JoinTask).

### Example of the Execution of Stage 2

#### Join produces the following table
```bash
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚   u.user_id â”‚ u.first_name   â”‚ u.last_name   â”‚   u.age â”‚ u.country   â”‚   o.order_id â”‚   o.user_id â”‚ o.product   â”‚   o.quantity â”‚   o.price â”‚ o.order_date        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           1 â”‚ Alice          â”‚ Smith         â”‚      25 â”‚ USA         â”‚            1 â”‚           1 â”‚ Laptop      â”‚            1 â”‚      1200 â”‚ 2025-01-01 00:00:00 â”‚
â”‚           2 â”‚ Bob            â”‚ Johnson       â”‚      30 â”‚ Canada      â”‚            2 â”‚           2 â”‚ Mouse       â”‚            2 â”‚        25 â”‚ 2025-01-05 00:00:00 â”‚
â”‚           3 â”‚ Charlie        â”‚ Brown         â”‚      22 â”‚ USA         â”‚            3 â”‚           3 â”‚ Keyboard    â”‚            1 â”‚        45 â”‚ 2025-02-10 00:00:00 â”‚
â”‚           1 â”‚ Alice          â”‚ Smith         â”‚      25 â”‚ USA         â”‚            4 â”‚           1 â”‚ Monitor     â”‚            2 â”‚       300 â”‚ 2025-03-15 00:00:00 â”‚
â”‚           4 â”‚ David          â”‚ Wilson        â”‚      35 â”‚ UK          â”‚            5 â”‚           4 â”‚ Laptop      â”‚            1 â”‚      1100 â”‚ 2025-03-20 00:00:00 â”‚
â”‚           5 â”‚ Eva            â”‚ Davis         â”‚      28 â”‚ Canada      â”‚            6 â”‚           5 â”‚ Mouse       â”‚            1 â”‚        30 â”‚ 2025-04-01 00:00:00 â”‚
â”‚           6 â”‚ Frank          â”‚ Miller        â”‚      40 â”‚ USA         â”‚            7 â”‚           6 â”‚ Keyboard    â”‚            2 â”‚        50 â”‚ 2025-04-10 00:00:00 â”‚
â”‚           7 â”‚ Grace          â”‚ Taylor        â”‚      27 â”‚ UK          â”‚            8 â”‚           7 â”‚ Monitor     â”‚            1 â”‚       280 â”‚ 2025-05-05 00:00:00 â”‚
â”‚           8 â”‚ Hank           â”‚ Anderson      â”‚      32 â”‚ USA         â”‚            9 â”‚           8 â”‚ Laptop      â”‚            1 â”‚      1300 â”‚ 2025-05-10 00:00:00 â”‚
â”‚           9 â”‚ Ivy            â”‚ Thomas        â”‚      26 â”‚ Canada      â”‚           10 â”‚           9 â”‚ Mouse       â”‚            3 â”‚        27 â”‚ 2025-06-01 00:00:00 â”‚
â”‚          10 â”‚ Jack           â”‚ Jackson       â”‚      24 â”‚ USA         â”‚           11 â”‚          10 â”‚ Keyboard    â”‚            1 â”‚        40 â”‚ 2025-06-15 00:00:00 â”‚
â”‚          11 â”‚ Kate           â”‚ White         â”‚      29 â”‚ UK          â”‚           12 â”‚          11 â”‚ Monitor     â”‚            2 â”‚       290 â”‚ 2025-07-01 00:00:00 â”‚
â”‚          12 â”‚ Leo            â”‚ Harris        â”‚      33 â”‚ USA         â”‚           13 â”‚          12 â”‚ Laptop      â”‚            1 â”‚      1250 â”‚ 2025-07-10 00:00:00 â”‚
â”‚          13 â”‚ Mia            â”‚ Martin        â”‚      31 â”‚ Canada      â”‚           14 â”‚          13 â”‚ Mouse       â”‚            2 â”‚        26 â”‚ 2025-07-15 00:00:00 â”‚
â”‚          14 â”‚ Nick           â”‚ Thompson      â”‚      23 â”‚ UK          â”‚           15 â”‚          14 â”‚ Keyboard    â”‚            1 â”‚        42 â”‚ 2025-08-01 00:00:00 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```
#### The Aggregate task groups by country and computes the aggregations
```bash
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ u.country   â”‚   orders_count â”‚   total_sales â”‚   _having_sum_o.quantity_mul_o.price â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Canada      â”‚              4 â”‚           213 â”‚                                  213 â”‚
â”‚ UK          â”‚              4 â”‚          2002 â”‚                                 2002 â”‚
â”‚ USA         â”‚              7 â”‚          4535 â”‚                                 4535 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```
#### The filter task filters out countries with total sales <= 500
```bash
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ u.country   â”‚   orders_count â”‚   total_sales â”‚   _having_sum_o.quantity_mul_o.price â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ UK          â”‚              4 â”‚          2002 â”‚                                 2002 â”‚
â”‚ USA         â”‚              7 â”‚          4535 â”‚                                 4535 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```
#### The project task renames the columns leading to the final output
```bash
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ u.country   â”‚   orders_count â”‚   total_sales â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ UK          â”‚              4 â”‚          2002 â”‚
â”‚ USA         â”‚              7 â”‚          4535 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```


### Code Generation

To speed up execution, the `ThreadPoolEngine` compiles the full plan to Zig code (via templating).
This code is then compiled once and executed on a thread pool natively, leading to significant speedups.
The communication between the Python threads and the native threads is done via files (shuffle files, output files) and `stdin`/`stdout` for jobs.

Here is a trace of the TPC-H benchmark (query 1, with 4 worker threads).

![Trace](trace.png)
